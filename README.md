# An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments

## Setup

1. Clone this repository
2. Install dependencies:
```
pip install -r requirements.txt
```

3. Create a `.env` file in the root directory with your API keys:
```
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

You can use the `example.env` file as a template - just rename it to `.env` and add your actual API keys.

## Running the Code

Run the generator with your preferred model:

```
python single_agent_factor.py --model gpt-4o-mini
```

Available model options:
- OpenAI models: `gpt-4o-mini`, `gpt-4o`
- Groq models: `llama3-8b-8192`, `llama3-70b-8192`, `meta-llama/llama-4-scout-17b-16e-instruct`, `meta-llama/llama-4-maverick-17b-128e-instruct`, `qwen-qwq-32b`, `deepseek-r1-distill-llama-70b`

## Security Note

The `.env` file is included in `.gitignore` to prevent API keys from being committed to version control. Never commit your actual API keys to the repository.

# Legal Argument Generation Pipeline

This project contains a pipeline for generating and evaluating legal arguments in trade secret misappropriation cases. The pipeline consists of several components:

1. **Argument Generation**: Uses AI agents to create structured legal arguments
2. **Score Calculation**: Evaluates the quality of the generated arguments

## Prerequisites

- Python 3.8+
- Required Python packages:
  - autogen
  - pandas
  - numpy
  - scipy
  - sqlite3
  - groq (optional)
  - openai

## Installation

```bash
pip install pandas numpy scipy autogen openai groq
```

## Usage

### Running the Complete Pipeline

The easiest way to run the entire pipeline is to use the `pipeline.py` script:

```bash
python pipeline.py --input-file data/non-arguable_factor_10_complexity5.csv
```

This will:
1. Run argument generation with the factor-based approach
2. Calculate scores and compare results

### Command-line Options

The pipeline script supports the following options:

- `--skip-generation`: Skip the argument generation step and process existing files
- `--model`: Select the model for argument generation (default: gpt-4o-mini)
- `--input-file`: Specify the input file with format `{mode}_factor_{number}_complexity{level}.csv`

Available model options:
- OpenAI models: `gpt-4o-mini`, `gpt-4o`
- Groq models: `llama3-8b-8192`, `llama3-70b-8192`, `meta-llama/llama-4-scout-17b-16e-instruct`, `meta-llama/llama-4-maverick-17b-128e-instruct`, `qwen-qwq-32b`, `deepseek-r1-distill-llama-70b`

Example:
```bash
# Run factor-based approach with Llama 3 70B on specific input file
python pipeline.py --model=llama3-70b-8192 --input-file data/arguable_factor_20_complexity4.csv

# Skip generation and process existing files for a specific input
python pipeline.py --skip-generation --input-file data/non-arguable_factor_10_complexity5.csv

# Run the factor-based approach
python pipeline.py --model=gpt-4o --input-file data/arguable_factor_10_complexity5.csv
```

### Input File Naming Convention

Input files should follow the naming convention:
```
{mode}_factor_{number}_complexity{level}.csv
```

Where:
- `mode`: The scenario type (non-arguable, reordered, arguable)
- `format`: This should always be `factor`.
- `number`: The number of scenarios in the file
- `level`: The complexity level of the scenarios

The pipeline automatically extracts this information from the filename and uses it throughout the process.

### Output File Structure

All output files preserve the input format information and follow a similar naming convention:
```
{type}_{mode}_factor_{number}_complexity{level}_{timestamp}.csv
```

For example, given an input file `non-arguable_factor_10_complexity5.csv`:
- `messages_non-arguable_factor_10_complexity5_20230615_123456.csv`
- `extracted_non-arguable_factor_10_complexity5_20230615_123456.csv`
- `comparison_non-arguable_factor_10_complexity5_20230615_123456.md`

This naming ensures full traceability between input scenarios and their corresponding results.

### Running Individual Components

If you prefer to run each step individually:

1. **Argument Generation**:
   ```bash
   # Use default model (gpt-4o-mini) with specific input file
   python single_agent_factor.py --input-file data/non-arguable_factor_10_complexity5.csv
   
   # Specify a different model with specific input file
   python single_agent_factor.py --model=llama3-70b-8192 --input-file data/arguable_factor_5_complexity8.csv
   ```

2. **Score Calculation**:
   ```bash
   # Using Score_Calculation.py on a messages_factor_...csv file generated by pipeline.py or single_agent_factor.py + extract_json_to_csv (within pipeline.py)
   python scores/Score_Calculation.py pipeline_results/your_model_name/your_scenario_name/messages_factor_your_scenario_name_timestamp.csv
   # Or, if score_calculation.py is run from its directory and the file is there:
   # python score_calculation.py ../pipeline_results/your_model_name/your_scenario_name/messages_factor_your_scenario_name_timestamp.csv
   ```

## Score Calculation

The `score_calculation.py` module evaluates the accuracy of factor identification in legal arguments:

- Examines common factors between input scenarios and test scenarios (TSC1 and TSC2)
- Calculates mismatches by checking:
  - Factors claimed as common but not present in both scenarios
  - Common factors not identified in the analysis
- Produces an overall accuracy percentage for each file
- The score calculation checks all common factors in every row, providing comprehensive analysis

## Directory Structure

- `data/`: Contains input data files
  - Generated input files with format `{mode}_factor_{number}_complexity{level}.csv`
- `pipeline_results/`: Contains all output from the pipeline, organized by model and mode
- `scores/`: Contains the scoring logic
- `*.py`: Implementation scripts

## Output Files

- `*_responses_*.db`: SQLite database containing raw responses
- `messages_*.csv`: CSV file with messages from the database
- `extracted_*.csv`: Extracted arguments and distilled factors
- `comparison_*.md`, `factor_report_*.md`: Markdown reports with results

## Model Configuration

The pipeline supports two types of models:

1. **OpenAI Models**:
   - `gpt-4o-mini`: A smaller, faster version of GPT-4o
   - `gpt-4o`: The full GPT-4o model with enhanced capabilities

2. **Groq-hosted Models**:
   - `llama3-8b-8192`: Llama 3 8B parameter model
   - `llama3-70b-8192`: Llama 3 70B parameter model
   - `meta-llama/llama-4-scout-17b-16e-instruct`: Llama 4 Scout 17B parameter model
   - `meta-llama/llama-4-maverick-17b-128e-instruct`: Llama 4 Maverick 17B parameter model
   - `qwen-qwq-32b`: Qwen QWQ 32B parameter model
   - `deepseek-r1-distill-llama-70b`: DeepSeek R1 Distill Llama 70B parameter model

Notes on model usage:
- Argument generation can use any of the above models (configurable)

## API Keys

API keys are managed in the agent scripts:
- OpenAI API key for GPT models
- Groq API key for Llama, Qwen, and DeepSeek models

If you plan to use these scripts, replace the API keys in the code with your own.

# Legal Scenario Generator

This tool generates legal scenarios with different modes and formats for testing argument evaluation capabilities.

## Overview

The Scenario Generator creates legal scenarios based on trade secret factors. Each scenario consists of:
- An input scenario with a set of factors
- Two test scenarios (TSC1 and TSC2) that have different outcomes (Plaintiff and Defendant)

## Usage

```bash
python data/scenario_generator.py [options]
```

### Command-line Arguments

- `--mode` - Type of scenario to generate:
  - `non-arguable` - Generates scenarios where test cases have no factors in common with the input scenario
  - `reordered` - Generates scenarios with certain reordered factors designed to be misleading
  - `arguable` - Generates scenarios with overlapping factors that can lead to legitimate arguments

- `--case-number` - Number of scenarios to generate (default: 10)

- `--complexity` - Controls the number of factors in each scenario (default: 5)
  - The actual number of factors will be randomly chosen between (complexity-1) and (complexity+1)

### Examples

Generate 10 non-arguable scenarios using factor names with complexity level 5:
```bash
python scenario_generator.py --mode non-arguable --case-number 10 --complexity 5
```

Generate 20 reordered scenarios with complexity level 4:
```bash
python scenario_generator.py --mode reordered --case-number 20 --complexity 4
```

Generate 5 arguable scenarios with higher complexity:
```bash
python scenario_generator.py --mode arguable --complexity 8 --case-number 5
```

## Output

The generator creates CSV files with the format `{mode}_factor_{number}_complexity{level}.csv` containing the scenarios.

When using the `code` output format, a separate `Factor_Mapping.json` file is generated to map codes back to their original factor names.

## Factor Types

Factors in the scenarios are categorized as:
- Pro-plaintiff factors (P) - Typically support the plaintiff's position
- Pro-defendant factors (D) - Typically support the defendant's position

## Modes Explained

1. **Non-arguable**: 
   - The input scenario and test scenarios have no common factors
   - Makes it difficult to argue for a particular outcome based on precedent

2. **Reordered**:
   - Introduces targeted reordered between input and test scenarios
   - TSC1 has some pro-defendant factors from the input, despite a pro-plaintiff outcome
   - TSC2 has some pro-plaintiff factors from the input, despite a pro-defendant outcome

3. **Arguable**:
   - TSC1 includes pro-plaintiff factors from the input
   - TSC2 includes pro-defendant factors from the input
   - These connections allow for legitimate argumentation based on factor similarity

## Internal Implementation

The Scenario Generator class provides several methods:
- `generate_input_factor()` - Creates factors for the input scenario
- `generate_tsc_factor()` - Creates factors for test scenarios based on the selected mode
- `update_tsc()` - Updates a test scenario with a new mode
- `restart()` - Regenerates the entire scenario
- `generate_initial_prompt()` - Creates the formatted scenario output

The complexity parameter controls factor count, with scenarios containing between (complexity-1) and (complexity+1) factors. Higher complexity values result in more factors in each scenario part.

## Pipeline Approach

The pipeline implements an approach for generating legal arguments using human-readable factor representations (e.g., "F1 Disclosure-in-negotiations (D)").
It generates arguments based on direct factor analysis and works with all supported models.
This approach is implemented in `single_agent_factor.py`.
